# Introduction to Reinforcement Learning ğŸ˜€

<p align="center"><img src="https://user-images.githubusercontent.com/66208800/173747708-e405fae0-fe2a-4356-a15d-c3477550a1c4.png"></p>

## Characteristics of Reinforcement Learning
+ ê¸°ê³„í•™ìŠµì—ëŠ” supervised learning, unsupervised learning, reinforcement learning ì´ ìˆë‹¤.
+ ê°•í™”í•™ìŠµì´ë€ supervisor, ì¦‰ ì •ë‹µì„ ì•Œë ¤ì£¼ëŠ” ì‚¬ëŒì´ ì—†ì´ agentê°€ ë¦¬ì›Œë“œë¥¼ ë°”íƒ•ìœ¼ë¡œ ìŠ¤ìŠ¤ë¡œ rewardë¥¼ maximize í•  ìˆ˜ ìˆëŠ” ì •ë‹µì„ ì°¾ì•„ê°€ëŠ” ê³¼ì •ì´ë‹¤.
+ feedbackì´ delayë  ìˆ˜ ìˆë‹¤. 
+ ë°ì´í„°ë¥¼ ì–´ë–»ê²Œ í”¼íŒ…í•˜ëŠëƒì— ë”°ë¼ì„œ ë°ì´í„°ë¥¼ ë°›ëŠ” ë°©ë²•ë„ ë‹¬ë¼ì§„ë‹¤. 

### Examples of Reinforcement Learning
+ íˆ¬ì í¬íŠ¸í´ë¦¬ì˜¤ ê´€ë¦¬ (reward : ì´ìœ¤) 
+ control power station
+ make a humanoid robot walk

### Reward
+ **cumulated reward**ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ê²ƒ, ì¦‰ ë‹¹ì¥ì˜ ë´…ìƒë³´ë‹¤ëŠ” ë¯¸ë˜ì˜ ë°›ê²Œ ë  ëª¨ë“  ë³´ìƒì˜ ì´ í•©ì„ maximize í•˜ëŠ” ê²ƒì´ agentì˜ ëª©ì ì´ë‹¤. 
+ **real hypothesis** : All goals can be described by the maximisation of expected cumulative reward
+ ex) ë¦¬ì›Œë“œë¥¼ maximize í•˜ëŠ” ê²ƒ = ì˜ ê±·ëŠ” ê²ƒ

### Agent and Environment
<p align="center"><img src="https://user-images.githubusercontent.com/66208800/173748500-e304716e-f21e-4898-a543-521929ed971b.png"></p>

+ ë§¤ stepë§ˆë‹¤ $A_t$ ì‹¤í–‰, $O_t$ ê´€ì°°, $R_t$ receive í•˜ëŠ” ê³¼ì • ë°˜ë³µ

### History and State
<p align="center"><img src="https://user-images.githubusercontent.com/66208800/173750281-6ff87be5-3128-4a93-b1d4-98de3fced73b.png"></p>

+ state ìì²´ëŠ” agentê°€ historyì˜ ì •ë³´ë¥¼ ê°€ê³µí•´ ë‹¤ìŒì˜ ìƒíƒœë¥¼ ê²°ì •í•˜ëŠ” í•¨ìˆ˜ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤.
+ **History** is the sequence of observations, actions, rewards


<p align="center"><img src="https://user-images.githubusercontent.com/66208800/173749953-7f5734a7-beed-437c-8fcd-570b0b3b576e.png"></p>

